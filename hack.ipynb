{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/ml-hackathon-ec-campus-set-1/train.csv', encoding='ISO-8859-1')\n",
    "# Define path to video clips\n",
    "video_dir = '/kaggle/input/ml-hackathon-ec-campus-set-1/train_videos'\n",
    "\n",
    "\n",
    "# Function to get video file path from IDs\n",
    "def get_video_clip_path(row):\n",
    "    dialogue_id = row['Dialogue_ID']\n",
    "    utterance_id = row['Utterance_ID']\n",
    "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    return os.path.join(video_dir, filename)\n",
    "\n",
    "# Apply the function to get file paths for each sampled clip\n",
    "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
    "\n",
    "# Check sample paths\n",
    "print(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to video clips\n",
    "df = pd.read_csv('/kaggle/input/ml-hackathon-ec-campus-set-1/test.csv', encoding='ISO-8859-1')\n",
    "video_dir = '/kaggle/input/ml-hackathon-ec-campus-set-1/test_videos'\n",
    "\n",
    "\n",
    "# Function to get video file path from IDs\n",
    "def get_video_clip_path(row):\n",
    "    dialogue_id = row['Dialogue_ID']\n",
    "    utterance_id = row['Utterance_ID']\n",
    "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    return os.path.join(video_dir, filename)\n",
    "\n",
    "# Apply the function to get file paths for each sampled clip\n",
    "df['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n",
    "\n",
    "# Check sample paths\n",
    "print(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller, kpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_df.head())\n",
    "sentences = train_df[[\"Utterance\",\"Sentiment\"]].values\n",
    "sentences = pd.DataFrame(sentences)\n",
    "\n",
    "sentences_test = df[\"Utterance\"].values\n",
    "sentences_test = pd.DataFrame(sentences_test)\n",
    "\n",
    "sentences.columns = ['Text', 'Sentiment']\n",
    "print(sentences)\n",
    "\n",
    "sentences_test.columns = ['Text']\n",
    "print(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from transformers import pipeline, BertModel\n",
    "\n",
    "# Use a model with 3 sentiment classes\n",
    "model = BertModel.from_pretrained\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "results = classifier(sentences)\n",
    "\n",
    "# Display results\n",
    "for sentence, result in zip(sentences, results):\n",
    "    print(f\"Sentence: {sentence}\\nSentiment: {result['label']} (Score: {result['score']:.2f})\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''results_df = pd.DataFrame(results)\n",
    "results_df['label'] = results_df['label'].map({'LABEL_2':1,'LABEL_0':2,'LABEL_1':2})\n",
    "print(results_df.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features (confidence score) and target (label)\n",
    "X = results_df[['score']]\n",
    "y = results_df['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "# Train the MLP model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Clean text data by removing stopwords and extra spaces\n",
    "sentences['Text'] = sentences['Text'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word.lower() not in ENGLISH_STOP_WORDS])\n",
    ")\n",
    "# Check if any rows are empty after cleaning\n",
    "empty_text = sentences[sentences['Text'].str.strip() == '']\n",
    "print(f\"Number of empty text rows: {empty_text.shape[0]}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Using TfidfVectorizer to convert text data to numerical features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "X = vectorizer.fit_transform(sentences['Text'])\n",
    "\n",
    "# Labels\n",
    "y = sentences['Sentiment']\n",
    "\n",
    "# Split into training and testing datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example DataFrame (replace with your actual sentences)\n",
    "# sentences = pd.DataFrame({'Text': [\"I love this!\", \"I hate that\", \"This is okay\"], 'Sentiment': ['positive', 'negative', 'neutral']})\n",
    "\n",
    "# Encode sentiments into numeric labels\n",
    "sentences['Sentiment'] = sentences['Sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 2})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(sentences, test_size=0.3, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create custom Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = sentences['Text'].values\n",
    "        self.labels = sentences['Sentiment'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encode text using BERT tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = SentimentDataset(train_data, tokenizer)\n",
    "test_dataset = SentimentDataset(test_data, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to GPU if available\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report for 3 sentiment classes\n",
    "print(classification_report(true_labels, predictions, target_names=['negative', 'positive', 'neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `test_data` contains only 'Text' column without the 'Sentiment' column\n",
    "# Replace the previous evaluation section with the following code\n",
    "\n",
    "test_sentences = sentences_test['Text'].values  # Only the text for prediction\n",
    "\n",
    "# Create a dataset for test data without labels\n",
    "class TestSentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Encode text using BERT tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# Prepare DataLoader for test data\n",
    "test_dataset = TestSentimentDataset(test_sentences, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Map predictions back to sentiment labels\n",
    "sentiment_labels = ['negative', 'positive', 'neutral']\n",
    "predicted_sentiments = [sentiment_labels[pred] for pred in predictions]\n",
    "\n",
    "all_preds = predicted_sentiments\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "        'ID': range(1,len(test_sentences)+1),\n",
    "        'Emotion': all_preds\n",
    "    })\n",
    "    \n",
    "# Save the DataFrame to CSV\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "# Path to the video directory\n",
    "video_dir = \"/kaggle/input/ml-hackathon-ec-campus-set-1/train\"  # Your video folder path\n",
    "\n",
    "# Create a directory to store audio files\n",
    "audio_dir = \"/kaggle/working/audio_files\"\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through all video files in the directory\n",
    "for filename in os.listdir(video_dir):\n",
    "    if filename.endswith(('.mp4', '.avi', '.mov')):  # Adjust extensions as needed\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        \n",
    "        # Load video file\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        \n",
    "        # Extract audio from the video\n",
    "        audio_clip = video_clip.audio\n",
    "        \n",
    "        # Save the audio file\n",
    "        audio_filename = os.path.splitext(filename)[0] + '.mp3'  # You can also use .wav\n",
    "        audio_path = os.path.join(audio_dir, audio_filename)\n",
    "        audio_clip.write_audiofile(audio_path)\n",
    "        \n",
    "        # Close clips after processing to free resources\n",
    "        video_clip.close()\n",
    "        audio_clip.close()\n",
    "\n",
    "print(\"Audio extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to your audio files directory\n",
    "audio_dir = \"/kaggle/working/audio_files\"  # Update with your path\n",
    "\n",
    "# Function to extract MFCC features from an audio file\n",
    "def extract_audio_features(audio_file):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Mean of the MFCC features across time\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    return mfcc_mean\n",
    "\n",
    "# Extract features for all audio files\n",
    "audio_features = []\n",
    "audio_filenames = []\n",
    "\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith('.mp3'):  # Adjust file extensions as needed\n",
    "        audio_file_path = os.path.join(audio_dir, filename)\n",
    "        features = extract_audio_features(audio_file_path)\n",
    "        audio_features.append(features)\n",
    "        audio_filenames.append(filename)\n",
    "\n",
    "# Convert to numpy array\n",
    "audio_features = np.array(audio_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with ISO-8859-1 encoding\n",
    "file_path = \"/kaggle/input/ml-hackathon-ec-campus-set-1/train.csv\"  # Replace with your actual CSV file path\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Preview the data to check the columns\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# Drop rows with missing 'Utterance' or 'Sentiment' columns\n",
    "df = df.dropna(subset=['Utterance', 'Sentiment'])\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df[['Utterance', 'Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment analysis pipeline using BERT\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to analyze sentiment of the utterance\n",
    "def analyze_text_sentiment(text):\n",
    "    result = sentiment_analyzer(text)\n",
    "    sentiment = result[0]['label']  # \"POSITIVE\" or \"NEGATIVE\"\n",
    "    return sentiment\n",
    "\n",
    "# Apply sentiment analysis to each utterance\n",
    "df['Predicted_Sentiment'] = df['Utterance'].apply(analyze_text_sentiment)\n",
    "\n",
    "# Map BERT sentiment output to numeric labels (positive: 1, negative: 0, neutral: 2)\n",
    "def sentiment_to_label(sentiment):\n",
    "    if sentiment == 'POSITIVE':\n",
    "        return 1\n",
    "    elif sentiment == 'NEGATIVE':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2  # Neutral\n",
    "\n",
    "df['Predicted_Sentiment_Label'] = df['Predicted_Sentiment'].apply(sentiment_to_label)\n",
    "\n",
    "# Preview the updated dataframe\n",
    "print(df[['Utterance', 'Sentiment', 'Predicted_Sentiment', 'Predicted_Sentiment_Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Convert actual sentiment to numeric labels (positive: 1, negative: 0, neutral: 2)\n",
    "def actual_sentiment_to_label(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 1\n",
    "    elif sentiment == 'negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2  # Neutral\n",
    "\n",
    "# Apply this function to the actual 'Sentiment' column\n",
    "df['Actual_Sentiment_Label'] = df['Sentiment'].apply(actual_sentiment_to_label)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(df['Actual_Sentiment_Label'], df['Predicted_Sentiment_Label'])\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"Classification Report:\\n\", classification_report(df['Actual_Sentiment_Label'], df['Predicted_Sentiment_Label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
