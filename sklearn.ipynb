{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcFpV68ed4vt"
      },
      "source": [
        "## **BEGINNER'S TUTORIAL ON SCIKIT-LEARN**\n",
        "\n",
        "In this tutorial, we will explore some common tasks that can be accomplished using scikit-learn, a popular machine learning package in Python. Scikit-learn is known for its simplicity and efficiency in handling various machine learning algorithms. We will cover the following topics:\n",
        "\n",
        "1. Loading a dataset\n",
        "2. Splitting the dataset into training, validation, and test sets\n",
        "3. Training different classification and regression models\n",
        "4. Finding missing values in the dataset\n",
        "5. Evaluating model performance using various metrics\n",
        "\n",
        "By the end of this tutorial, you will have a good understanding of how to use scikit-learn to build and evaluate machine learning models. Let's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDVfveIRaFiT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_KKfljdd4vu"
      },
      "source": [
        "### Package Installation & Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tIJ3WHpGd4vu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.1)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
            "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aakan\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\aakan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
            "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 1.0/12.6 MB 8.4 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.6/12.6 MB 3.8 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 2.4/12.6 MB 4.3 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 2.4/12.6 MB 4.3 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 3.4/12.6 MB 3.3 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 3.9/12.6 MB 3.2 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 4.7/12.6 MB 3.2 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 5.5/12.6 MB 3.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 6.0/12.6 MB 3.2 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 6.3/12.6 MB 3.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.6/12.6 MB 3.0 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 7.1/12.6 MB 2.9 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 7.6/12.6 MB 2.8 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.9/12.6 MB 2.8 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 8.1/12.6 MB 2.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.7/12.6 MB 2.6 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.7/12.6 MB 2.6 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 9.2/12.6 MB 2.4 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.4/12.6 MB 2.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 10.2/12.6 MB 2.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 10.2/12.6 MB 2.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.5/12.6 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 11.0/12.6 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 11.3/12.6 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.5/12.6 MB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.8/12.6 MB 2.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.1/12.6 MB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.3/12.6 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.6/12.6 MB 2.1 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (C:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (C:\\Python312\\Lib\\site-packages)\n",
            "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
            "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python312\\\\Scripts\\\\f2py.exe' -> 'C:\\\\Python312\\\\Scripts\\\\f2py.exe.deleteme'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#execute this cell to install the required packages (if not done already)\n",
        "! pip install scikit-learn numpy pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKkBUTMd4vv"
      },
      "source": [
        "#### Install Kaggle API\n",
        "You need to have the Kaggle API installed. You can install it using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l58EFSItd4vv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py egg_info did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [11 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"<string>\", line 2, in <module>\n",
            "        File \"<pip-setuptools-caller>\", line 14, in <module>\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
            "          import setuptools.version\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
            "          import pkg_resources\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2191, in <module>\n",
            "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
            "                          ^^^^^^^^^^^^^^^^^^^\n",
            "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFy0lCzd4vv"
      },
      "source": [
        "#### Set Up Kaggle API Credentials\n",
        "1. Go to Kaggle's website and sign up.\n",
        "2. Go to \"My Account\" (click on your profile picture in the top right corner and then on \"My Account\").\n",
        "3. Go to \"Settings\"\n",
        "4. Scroll down to the \"API\" section and click on \"Create New API Token\". This will download a file called kaggle.json.\n",
        "5. Place the kaggle.json file in the .kaggle directory in your home directory. You can do this with the following commands:\n",
        "```sh\n",
        "    mkdir -p ~/.kaggle\n",
        "    mv /path/to/kaggle.json ~/.kaggle/\n",
        "    chmod 600 ~/.kaggle/kaggle.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QTYh2j3eeYw2"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m files\u001b[38;5;241m.\u001b[39mupload()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIUfQv18edDd"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as3kURRTJS9T"
      },
      "outputs": [],
      "source": [
        "!ls -a /root/.kaggle #to check whethere its actually there or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgCrgcrneg2W"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrF5XibdekWb"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c house-prices-advanced-regression-techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz_a783Ad4vw"
      },
      "source": [
        "<!-- #### Joining Relevant Competition on Kaggle\n",
        "1. Make sure you have logged into kaggle with the same account the API key has been generated for.\n",
        "2. Go to https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques and click on \"Join Competition\".\n",
        "3. This way, the regression dataset (House Price Prediction) will download seamlessly. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUSds7oJd4vw"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OjpNjV-d4vw"
      },
      "source": [
        "#### Download the Dataset\n",
        "Now you can use the Kaggle API to download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch6XZz3Fd4vw"
      },
      "outputs": [],
      "source": [
        "filename = 'breast-cancer-wisconsin-data.zip'\n",
        "os.makedirs(\"classification\", exist_ok=True)\n",
        "for root, dirs, file in os.walk(\"/content\", topdown=True):\n",
        "    if filename in file:\n",
        "        break\n",
        "else:\n",
        "    !kaggle datasets download -d uciml/breast-cancer-wisconsin-data\n",
        "    !mv breast-cancer-wisconsin-data.zip 'classification/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHlQyNc9d4vw"
      },
      "outputs": [],
      "source": [
        "for root, dirs, file in os.walk(\"/content/classification/\", topdown=True):\n",
        "    if 'data.csv' in file:\n",
        "        break\n",
        "else:\n",
        "    !unzip /content/classification/breast-cancer-wisconsin-data.zip -d /content/classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8BIYr2ykd4vw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81NmA4BFd4vw"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcK5yqovd4vw"
      },
      "source": [
        "#### 1. Loading a dataset\n",
        "The breast cancer dataset, provided by scikit-learn, is a widely used dataset in the field of machine learning and data science. This dataset contains measurements of various features of cell nuclei present in breast cancer biopsies. It is commonly used for binary classification tasks to distinguish between malignant (cancerous) and benign (non-cancerous) tumors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vGfIIzAtd4vw"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'classification/data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classification/data.csv'"
          ]
        }
      ],
      "source": [
        "data = \"classification/data.csv\"\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkI3pjxwd4vw"
      },
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndIRHa3Rd4vw"
      },
      "source": [
        "#### 2. Splitting the dataset into training, validation, and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7E1amOzd4vw"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"Display summary statistics: \\n\",df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJA9BI7Hd4vx"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"missing values:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh8VswGcd4vx"
      },
      "source": [
        "### Data Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpgbfn8Ad4vx"
      },
      "outputs": [],
      "source": [
        "#Drop columns which\n",
        "\n",
        "df = df.drop(['Unnamed: 32', 'id'], axis = 1)\n",
        "\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' column to binary\n",
        "y = y.replace({'M': 1, 'B': 0})\n",
        "\n",
        "print(\"target: \\n\",y.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KnBqWujd4vx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training, validation, and test sets (70%, 10%, 20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
        "\n",
        "# Check the shapes of the splits\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLkbyIICd4vx"
      },
      "source": [
        "#### 3. Training different classification models\n",
        "In this section, we will demonstrate how to initialize and train different classification models using scikit-learn. While we won't go into the detailed workings of these models, it's important to know that there are multiple algorithms available for classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgualnKpd4vx"
      },
      "outputs": [],
      "source": [
        "# Initialize and train a Logistic Regression Model\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "log_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAoCte_xd4vx"
      },
      "outputs": [],
      "source": [
        "# Initialize and train DecisionTree Model\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoQuIxKId4vx"
      },
      "outputs": [],
      "source": [
        "# Initialize and train SVM Model\n",
        "svm_clf = SVC(random_state=42)\n",
        "svm_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm4Pz4djd4vx"
      },
      "source": [
        "If you wish to look at the predictions of each model separately, try executing `model_name.predict(X_val)`.\n",
        "\n",
        "These predictions are then compared to `y_val` for better insigths at how the model is performing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUSlPco4d4vx"
      },
      "source": [
        "#### 4. Visualizing the metrics for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xq807hYd4vx"
      },
      "outputs": [],
      "source": [
        "# Summary of performance metrics\n",
        "metrics = {\n",
        "    'Model': ['Logistic Regression', 'Decision Tree', 'SVM'],\n",
        "    'Accuracy': [accuracy_score(y_val, log_reg.predict(X_val)),\n",
        "                 accuracy_score(y_val, tree_clf.predict(X_val)),\n",
        "                 accuracy_score(y_val, svm_clf.predict(X_val))],\n",
        "    'Precision': [precision_score(y_val, log_reg.predict(X_val)),\n",
        "                  precision_score(y_val, tree_clf.predict(X_val)),\n",
        "                  precision_score(y_val, svm_clf.predict(X_val))],\n",
        "    'Recall': [recall_score(y_val, log_reg.predict(X_val)),\n",
        "               recall_score(y_val, tree_clf.predict(X_val)),\n",
        "               recall_score(y_val, svm_clf.predict(X_val))],\n",
        "    'F1-Score': [f1_score(y_val, log_reg.predict(X_val)),\n",
        "                 f1_score(y_val, tree_clf.predict(X_val)),\n",
        "                 f1_score(y_val, svm_clf.predict(X_val))]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8s1rVQZd4vx"
      },
      "source": [
        "Based on the performance metrics, it appears that the **Logistic Regression** model is the best fit for this data. It achieved the highest accuracy, precision , recall, and F1-score are also superior compared to the other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QWMmJDid4vy"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVDiyAvbd4vy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "filename = '/content/house-prices-advanced-regression-techniques.zip'\n",
        "os.makedirs('regression', mode=0o777, exist_ok=True)\n",
        "for root, dirs, file in os.walk(\"./content\", topdown=True):\n",
        "    if filename in file:\n",
        "        break\n",
        "else:\n",
        "    !kaggle competitions download -c house-prices-advanced-regression-techniques\n",
        "    !mv house-prices-advanced-regression-techniques.zip regression/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgMVmNwpd4vy"
      },
      "outputs": [],
      "source": [
        "if len(os.listdir('/content/regression')) == 1:\n",
        "    ! unzip '/content/regression/house-prices-advanced-regression-techniques.zip' -d '/content/regression'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNayG75ad4vy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/regression/train.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E3YAr1gd4vy"
      },
      "outputs": [],
      "source": [
        "# Define the target variable and features\n",
        "target = 'SalePrice'\n",
        "features = df.drop(columns=[target])\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n51nHVi0d4vy"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing target values\n",
        "df = df.dropna(subset=[target])\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXKc1jWFd4vy"
      },
      "outputs": [],
      "source": [
        "# drop columns with all NaN's\n",
        "df = df.dropna(axis=1)\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEP-q7Pzd4vy"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocess the data: scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN6ay9f7d4vy"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPeKbI9fd4v1"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVSuOxaLd4v1"
      },
      "outputs": [],
      "source": [
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNe-kVoRd4v1"
      },
      "outputs": [],
      "source": [
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R²): {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNXnZC7Ld4v2"
      },
      "outputs": [],
      "source": [
        "if r2 > 0.8:\n",
        "    print(\"The model explains a high proportion of the variance in house prices, suggesting a strong fit.\")\n",
        "elif r2 > 0.5:\n",
        "    print(\"The model explains a moderate proportion of the variance in house prices, indicating a reasonable fit.\")\n",
        "else:\n",
        "    print(\"The model explains a low proportion of the variance in house prices, indicating that it may not fit the data well.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BSsYCxhd4v2"
      },
      "source": [
        "## That's the end of this notebook, hope you had a fun learning experience!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
